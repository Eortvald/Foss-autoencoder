{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## FOSS Deep Learning Project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ran\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch, torchvision\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import *\n",
    "import autoencoder, classifier, preprocess\n",
    "from autoencoder.CAE_model import *\n",
    "import os\n",
    "\n",
    "abs_dir = os.path.abspath('')\n",
    "\n",
    "full_path = os.path.join(abs_dir, 'relative/path/to/file/you/want')\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('ran')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Preproccesing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    }
   ],
   "source": [
    "print('test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "train_loader = 2\n",
    "test_loader = 2\n",
    "b_size = 4\n",
    "num_epochs = 10\n",
    "learning_rate = 1e-3\n",
    "\n",
    "# dimension of the hidden layers\n",
    "layer_channels = [8, 16, 32]\n",
    "z_dim = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['3 X 32 X 32']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nTENSOR = torch.rand(128,9,9)\\nbox = torch.tensor(data)\\nprint(TENSOR, box.shape,4*\"\\n\")\\nflad = torch.flatten(TENSOR)\\nprint(\\'New shape: \\n\\',flad.shape)\\n'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def conv2D_out_dim(chan, in_dim, kernel, stride, padding, mode = 'Normal'):\n",
    "\n",
    "    if mode == 'Normal':\n",
    "        out_dim = ((in_dim + 2 * padding - 1 * (kernel-1))/stride)+1\n",
    "    #out_dimW = in_dimH if in_dimW is None else in_dimH\n",
    "    else:\n",
    "        out_dim = (in_dim - 1) * stride - 2 * padding + (kernel - 1) + 1\n",
    "        \n",
    "    return [f'{chan} X {out_dim} X {out_dim}']\n",
    "\n",
    "print(conv2D_out_dim(chan = 3,\n",
    "                     in_dim= 17,\n",
    "                     kernel= 2,\n",
    "                     stride= 2,\n",
    "                     padding=1,\n",
    "                     mode = 'Transpose'))\n",
    "\n",
    "#prediction = SimpleVAE(data)\n",
    "#print('Autoencoder are awesome')\n",
    "data = [[[11,22],\n",
    "         [33,44]],\n",
    "        [[55,66],\n",
    "         [77,88]],\n",
    "        [[99,10],\n",
    "         [11,12]]]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "TENSOR = torch.rand(128,9,9)\n",
    "box = torch.tensor(data)\n",
    "print(TENSOR, box.shape,4*\"\\n\")\n",
    "flad = torch.flatten(TENSOR)\n",
    "print('New shape: \\n',flad.shape)\n",
    "\"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple ANN\n",
    "###### Classifier taking Feature vectors from autoencoder as input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.1208, -0.9813, -0.8465,  1.9548,  2.1905,  2.3003, -2.0297,  0.2910,\n",
      "         -0.0086,  0.2439,  0.9227, -1.4990, -1.9170,  2.1368, -1.9473, -0.1724,\n",
      "          0.7389, -0.4259, -1.7884, -0.9788,  1.9060,  0.6895,  1.9558, -0.6700,\n",
      "          0.1882, -0.4031, -0.2615, -0.4290,  1.1994, -0.0938]],\n",
      "       device='cuda:0', grad_fn=<AddmmBackward>)\n"
     ]
    }
   ],
   "source": [
    "X = torch.ones([1, 8,200,89]).to(device)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "aemodel = CAE(z_dim=30).to(device)\n",
    "aemodel.load_state_dict(torch.load('./autoencoder/model_dicts/CAE_10Kmodel.pth', map_location=device))\n",
    "aemodel.eval()\n",
    "\n",
    "ENCO = lambda img : aemodel.encode(img)\n",
    "print(ENCO(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Images loaded: [0/8000]  ------  19:36:52\n",
      "Images loaded: [200/8000]  ------  19:36:52\n",
      "Images loaded: [400/8000]  ------  19:36:53\n",
      "Images loaded: [600/8000]  ------  19:36:53\n",
      "Images loaded: [800/8000]  ------  19:36:54\n",
      "Images loaded: [1000/8000]  ------  19:36:54\n",
      "Images loaded: [1200/8000]  ------  19:36:55\n",
      "Images loaded: [1400/8000]  ------  19:36:55\n",
      "Images loaded: [1600/8000]  ------  19:36:56\n",
      "Images loaded: [1800/8000]  ------  19:36:56\n",
      "Images loaded: [2000/8000]  ------  19:36:57\n",
      "Images loaded: [2200/8000]  ------  19:36:57\n",
      "Images loaded: [2400/8000]  ------  19:36:57\n",
      "Images loaded: [2600/8000]  ------  19:36:58\n",
      "Images loaded: [2800/8000]  ------  19:36:58\n",
      "Images loaded: [3000/8000]  ------  19:36:59\n",
      "Images loaded: [3200/8000]  ------  19:36:59\n",
      "Images loaded: [3400/8000]  ------  19:37:00\n",
      "Images loaded: [3600/8000]  ------  19:37:00\n",
      "Images loaded: [3800/8000]  ------  19:37:01\n",
      "Images loaded: [4000/8000]  ------  19:37:01\n",
      "Images loaded: [4200/8000]  ------  19:37:02\n",
      "Images loaded: [4400/8000]  ------  19:37:02\n",
      "Images loaded: [4600/8000]  ------  19:37:02\n",
      "Images loaded: [4800/8000]  ------  19:37:03\n",
      "Images loaded: [5000/8000]  ------  19:37:03\n",
      "Images loaded: [5200/8000]  ------  19:37:04\n",
      "Images loaded: [5400/8000]  ------  19:37:04\n",
      "Images loaded: [5600/8000]  ------  19:37:05\n",
      "Images loaded: [5800/8000]  ------  19:37:05\n",
      "Images loaded: [6000/8000]  ------  19:37:06\n",
      "Images loaded: [6200/8000]  ------  19:37:06\n",
      "Images loaded: [6400/8000]  ------  19:37:07\n",
      "Images loaded: [6600/8000]  ------  19:37:07\n",
      "Images loaded: [6800/8000]  ------  19:37:07\n",
      "Images loaded: [7000/8000]  ------  19:37:08\n",
      "Images loaded: [7200/8000]  ------  19:37:08\n",
      "Images loaded: [7400/8000]  ------  19:37:09\n",
      "Images loaded: [7600/8000]  ------  19:37:09\n",
      "Images loaded: [7800/8000]  ------  19:37:10\n",
      "Done reading train/ images\n",
      "Beginning permute\n",
      "Finished permute\n",
      "Dimension of X is :torch.Size([8000, 8, 200, 89])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Beginning Norm transform\n",
      "Norm transform finished\n",
      "Dimension of X is :torch.Size([8000, 8, 200, 89])------------\n",
      "tensor([[[-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         ...,\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820]],\n",
      "\n",
      "        [[-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         ...,\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317]],\n",
      "\n",
      "        [[-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         ...,\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         ...,\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622]],\n",
      "\n",
      "        [[-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         ...,\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445]],\n",
      "\n",
      "        [[-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         ...,\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690]]])\n",
      "Images loaded: [0/2000]  ------  19:39:34\n",
      "Images loaded: [200/2000]  ------  19:39:34\n",
      "Images loaded: [400/2000]  ------  19:39:35\n",
      "Images loaded: [600/2000]  ------  19:39:35\n",
      "Images loaded: [800/2000]  ------  19:39:36\n",
      "Images loaded: [1000/2000]  ------  19:39:36\n",
      "Images loaded: [1200/2000]  ------  19:39:37\n",
      "Images loaded: [1400/2000]  ------  19:39:37\n",
      "Images loaded: [1600/2000]  ------  19:39:37\n",
      "Images loaded: [1800/2000]  ------  19:39:38\n",
      "Done reading test/ images\n",
      "Beginning permute\n",
      "Finished permute\n",
      "Dimension of X is :torch.Size([2000, 8, 200, 89])\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "Beginning Norm transform\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Norm transform finished\n",
      "Dimension of X is :torch.Size([2000, 8, 200, 89])------------\n",
      "tensor([[[-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         ...,\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820],\n",
      "         [-0.4820, -0.4820, -0.4820,  ..., -0.4820, -0.4820, -0.4820]],\n",
      "\n",
      "        [[-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         ...,\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317],\n",
      "         [-0.5317, -0.5317, -0.5317,  ..., -0.5317, -0.5317, -0.5317]],\n",
      "\n",
      "        [[-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         ...,\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315],\n",
      "         [-0.5315, -0.5315, -0.5315,  ..., -0.5315, -0.5315, -0.5315]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         ...,\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622],\n",
      "         [-0.5622, -0.5622, -0.5622,  ..., -0.5622, -0.5622, -0.5622]],\n",
      "\n",
      "        [[-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         ...,\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445],\n",
      "         [-0.5445, -0.5445, -0.5445,  ..., -0.5445, -0.5445, -0.5445]],\n",
      "\n",
      "        [[-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         ...,\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690],\n",
      "         [-0.5690, -0.5690, -0.5690,  ..., -0.5690, -0.5690, -0.5690]]])\n",
      "Using cuda device\n",
      "\n",
      "\t\t------------------------------Epoch: 1------------------------------\n",
      "Avg train loss: 0.020931977301836015\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0052085741460323335  | Accuray: 1.8\n",
      "\n",
      "\t\t------------------------------Epoch: 2------------------------------\n",
      "Avg train loss: 0.020816477984189986\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005200929999351502  | Accuray: 2.35\n",
      "\n",
      "\t\t------------------------------Epoch: 3------------------------------\n",
      "Avg train loss: 0.020791678696870805\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005195351868867874  | Accuray: 2.55\n",
      "\n",
      "\t\t------------------------------Epoch: 4------------------------------\n",
      "Avg train loss: 0.020771239161491394\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0051906765401363375  | Accuray: 2.75\n",
      "\n",
      "\t\t------------------------------Epoch: 5------------------------------\n",
      "Avg train loss: 0.020753188759088517\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005186188876628876  | Accuray: 2.8\n",
      "\n",
      "\t\t------------------------------Epoch: 6------------------------------\n",
      "Avg train loss: 0.02073544842004776\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005181838631629944  | Accuray: 2.95\n",
      "\n",
      "\t\t------------------------------Epoch: 7------------------------------\n",
      "Avg train loss: 0.02071719506382942\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005176821053028107  | Accuray: 3.2\n",
      "\n",
      "\t\t------------------------------Epoch: 8------------------------------\n",
      "Avg train loss: 0.020696173131465913\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005169990569353104  | Accuray: 3.85\n",
      "\n",
      "\t\t------------------------------Epoch: 9------------------------------\n",
      "Avg train loss: 0.020667601585388183\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005160220205783844  | Accuray: 4.55\n",
      "\n",
      "\t\t------------------------------Epoch: 10------------------------------\n",
      "Avg train loss: 0.020622511237859725\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005144567370414734  | Accuray: 5.9\n",
      "\n",
      "\t\t------------------------------Epoch: 11------------------------------\n",
      "Avg train loss: 0.02054869621992111\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005122720628976822  | Accuray: 6.95\n",
      "\n",
      "\t\t------------------------------Epoch: 12------------------------------\n",
      "Avg train loss: 0.020454794198274614\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005097961515188217  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 13------------------------------\n",
      "Avg train loss: 0.020354909539222716\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005073101386427879  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 14------------------------------\n",
      "Avg train loss: 0.02025453518331051\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005047518834471703  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 15------------------------------\n",
      "Avg train loss: 0.020149628147482872\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.005021275147795677  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 16------------------------------\n",
      "Avg train loss: 0.02004573157429695\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.00499597042798996  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 17------------------------------\n",
      "Avg train loss: 0.019948009505867958\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004973616316914558  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 18------------------------------\n",
      "Avg train loss: 0.01986413735151291\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004954503372311592  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 19------------------------------\n",
      "Avg train loss: 0.01979401245713234\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004939547792077064  | Accuray: 7.45\n",
      "\n",
      "\t\t------------------------------Epoch: 20------------------------------\n",
      "Avg train loss: 0.01973967815935612\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004927364632487297  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 21------------------------------\n",
      "Avg train loss: 0.019695980802178385\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004917275130748749  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 22------------------------------\n",
      "Avg train loss: 0.019661140233278276\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0049092060625553135  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 23------------------------------\n",
      "Avg train loss: 0.01963302989304066\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004902851104736328  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 24------------------------------\n",
      "Avg train loss: 0.01961085119843483\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004897678032517433  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 25------------------------------\n",
      "Avg train loss: 0.019591478660702707\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004893506243824959  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 26------------------------------\n",
      "Avg train loss: 0.019575923174619675\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg. test loss 0.004889924213290215  | Accuray: 7.4\n",
      "\n",
      "\t\t------------------------------Epoch: 27------------------------------\n",
      "Avg train loss: 0.01956255812942982\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004886930361390113  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 28------------------------------\n",
      "Avg train loss: 0.019551299214363098\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048843643814325335  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 29------------------------------\n",
      "Avg train loss: 0.019541864454746247\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048820717930793765  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 30------------------------------\n",
      "Avg train loss: 0.01953299666941166\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004880003780126572  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 31------------------------------\n",
      "Avg train loss: 0.01952518913149834\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004878423169255256  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 32------------------------------\n",
      "Avg train loss: 0.019518822073936463\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.00487702439725399  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 33------------------------------\n",
      "Avg train loss: 0.019512444138526916\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004875696316361427  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 34------------------------------\n",
      "Avg train loss: 0.019506893932819368\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004874450698494911  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 35------------------------------\n",
      "Avg train loss: 0.019501798436045648\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004873391076922417  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 36------------------------------\n",
      "Avg train loss: 0.01949851383268833\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048726401776075364  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 37------------------------------\n",
      "Avg train loss: 0.019493345513939858\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004871648520231247  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 38------------------------------\n",
      "Avg train loss: 0.019489566177129745\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048707487732172015  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 39------------------------------\n",
      "Avg train loss: 0.0194859434068203\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004870158851146698  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 40------------------------------\n",
      "Avg train loss: 0.01948269368708134\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004869536131620407  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 41------------------------------\n",
      "Avg train loss: 0.019479857847094537\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004869003891944885  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 42------------------------------\n",
      "Avg train loss: 0.01947701604664326\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048683472275733945  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 43------------------------------\n",
      "Avg train loss: 0.019474112316966056\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004867842316627503  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 44------------------------------\n",
      "Avg train loss: 0.019471701219677926\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.0048674106895923615  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 45------------------------------\n",
      "Avg train loss: 0.019469458654522896\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004866993770003319  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 46------------------------------\n",
      "Avg train loss: 0.01946689900755882\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004866586819291115  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 47------------------------------\n",
      "Avg train loss: 0.01946464116871357\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004866073757410049  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 48------------------------------\n",
      "Avg train loss: 0.019462448373436927\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004865715354681015  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 49------------------------------\n",
      "Avg train loss: 0.019460523694753647\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.00486547639966011  | Accuray: 7.35\n",
      "\n",
      "\t\t------------------------------Epoch: 50------------------------------\n",
      "Avg train loss: 0.019458207666873933\n",
      "\t\t\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<\n",
      "Avg. test loss 0.004865060672163963  | Accuray: 7.35\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from classifier.MLP_Classifier import *\n",
    "hidden_out = [8, 10, 8]\n",
    "ANN_10Kmodel = ANN(30, hidden_out)\n",
    "ANN_10Kmodel = ANN_10Kmodel.to(device)\n",
    "learningrate = 0.001  # Insert LR\n",
    "epochs = 50  # Insert epochs\n",
    "\n",
    "train_log = []\n",
    "test_log = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f'\\n\\t\\t------------------------------Epoch: {epoch + 1}------------------------------')\n",
    "    tr_loss = train_model(traindataloader, ANN_10Kmodel, ENCO)\n",
    "    train_log.append(tr_loss)\n",
    "    \n",
    "    print('\\t\\t\\t>>>>>>>>>>>>>>>>TEST RESULTS<<<<<<<<<<<<<<<<<')\n",
    "    te_loss = model_evaluate(testdataloader, ANN_10Kmodel, ENCO)\n",
    "    test_log.append(te_loss)\n",
    "    \n",
    "torch.save(ANN_10Kmodel.state_dict(), 'classifier/model_dicts/ANN_10Kmodel.pth')\n",
    "\n",
    "img_name = f\"plots/classifier-plots/ANN_10K_Results-{str(datetime.now())[5:-10].replace(' ', '_').replace(':', '-')}.png\"\n",
    "plt.plot(np.arange(len(train_log)), train_log, label='Train')  # etc.\n",
    "plt.plot(np.arange(len(test_log)), test_log, label='Test')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title(\"Train vs Test loss\")\n",
    "plt.legend()\n",
    "plt.savefig(img_name, transparent = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
